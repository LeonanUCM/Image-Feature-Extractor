{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"toc_visible":true},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7659340,"sourceType":"datasetVersion","datasetId":4464607}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/leonanvasconcelos/image-feature-extractor?scriptVersionId=163481169\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Image Feature Extractor\n\nExtracting and displaying image features such as texture, contrast, hue, saturation, color, and homogeneity \ninvolves using specific image processing techniques and libraries like OpenCV and scikit-image. <br/>\nHere's a Jupyter notebook in Python that demonstrates how to extract some of these features and display them for human interpretation. <br/>\nThis processing is meant to be a starting point for feature extraction and analysis.<br/>\n<br/>\n<br/>\n**Author:** Leonan Vasconcelos<br/>\n19/feb/2024","metadata":{"id":"P_Jremwq3LMj"}},{"cell_type":"code","source":"# Kaggle specific code: takes a random file from a random Dataset\nimport os\nimport random\n\n# Select a random dataset\nkaggle_path = '/kaggle/input/'\ndirectories = [file for file in os.listdir(kaggle_path) if os.path.isdir(kaggle_path)]\nrandom_dataset = random.choice(directories)\ndirectory = os.path.join(kaggle_path, random_dataset) \nprint(f'Selected dataset: {random_dataset}')\n\n\n# Select a random file\nfiles = [file for file in os.listdir(directory) if os.path.isfile(os.path.join(directory, file))]\nrandom_file = random.choice(files)\npath_image = os.path.join(directory, random_file)\nprint(f'Selected image: {path_image}')","metadata":{"execution":{"iopub.status.busy":"2024-02-19T23:02:37.016246Z","iopub.execute_input":"2024-02-19T23:02:37.016653Z","iopub.status.idle":"2024-02-19T23:02:37.029075Z","shell.execute_reply.started":"2024-02-19T23:02:37.016623Z","shell.execute_reply":"2024-02-19T23:02:37.027651Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Requirements","metadata":{}},{"cell_type":"code","source":"from skimage.feature import graycomatrix, graycoprops\nfrom IPython.display import Image\nimport numpy as np\nimport cv2\nfrom PIL import Image as PILImage\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom skimage.filters import threshold_otsu","metadata":{"id":"SH1_vE0XOBEL","execution":{"iopub.status.busy":"2024-02-19T23:02:37.037453Z","iopub.execute_input":"2024-02-19T23:02:37.037922Z","iopub.status.idle":"2024-02-19T23:02:37.046409Z","shell.execute_reply.started":"2024-02-19T23:02:37.03789Z","shell.execute_reply":"2024-02-19T23:02:37.045036Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Utils\n\nBelow are support functions for better presentation of results.\nThey don't affect the analisys itself.","metadata":{}},{"cell_type":"code","source":"def resize_image(image, max_resolution=1200, downscale_factor=0):\n    \"\"\"\n    Resizes an image while maintaining the aspect ratio.\n\n    Args:\n        image: The image to be resized.\n        max_resolution: The maximum size for the longest dimension of the image.\n        downscale_factor: Factor by which the image dimensions are divided.\n\n    Returns:\n        The resized image and the scaling factor.\n    \"\"\"\n    height, width = image.shape[:2]\n\n    if downscale_factor > 0:\n        scaling_factor = 1 / downscale_factor\n    elif max_resolution > 0:\n        scaling_factor = max_resolution / max(height, width)\n    else:\n        scaling_factor = 1\n\n    new_height, new_width = int(height * scaling_factor), int(width * scaling_factor)\n\n    # Resize and return the image along with the scaling factor\n    # INTER_LANCZOS4 is the method that better preservers the texture\n    return cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LANCZOS4), scaling_factor\n\ndef show_images(*images, titles=[], figsize=8, max_resolution=400):\n    \"\"\"\n    Displays images in a single row of subplots with optional titles and labels reflecting the original image size.\n\n    Args:\n        *images: A sequence of images to display. Can be either numpy arrays or string paths to the images.\n        titles: A list of titles for each image. Default is None.\n        figsize: Size of the figure for displaying the images.\n        max_resolution: The maximum resolution to be displayed.\n\n    Returns:\n        None\n    \"\"\"\n    plt.figure(figsize=(figsize * len(images), figsize))\n\n    for i, name_or_image in enumerate(images):\n        # If the input is a string, assume it's a path to an image\n        if isinstance(name_or_image, str):\n            image = cv2.imread(name_or_image)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            image = np.array(image)\n        else:\n            image = name_or_image\n\n        height, width = image.shape[:2]\n\n        # Skip images with invalid dimensions\n        if height == 0 or width == 0:\n            continue\n\n        # Convert shape (3, 4, 1) to (3, 4)\n        if image.ndim == 3:\n            channels = image.shape[2]\n            if channels == 1:\n                image = np.squeeze(image, axis=-1)\n\n        # If the image is grayscale, convert it to RGB\n        if image.ndim == 2:\n            image = np.stack((image,) * 3, axis=-1)\n\n        # Resize the image if it exceeds the maximum resolution\n        image, scaling_factor = resize_image(image, max_resolution)\n\n        # Create a subplot for the image\n        ax = plt.subplot(1, len(images), i + 1)\n        plt.imshow(image)\n\n        # Set the title of the subplot\n        title = titles[i] if titles else f\"Image {i + 1}\"\n        ax.set_title(f\"{title}\", fontsize=20) \n        plt.text(0.5, -0.1, f\"{width}x{height}px\", ha='center', va='top', transform=plt.gca().transAxes)\n\n        # Update tick labels to reflect original image dimensions\n        def format_func_x(value, tick_number, scaling_factor=scaling_factor):\n            return f\"{int(value / scaling_factor)}\"\n        def format_func_y(value, tick_number, scaling_factor=scaling_factor):\n            return f\"{int(value / scaling_factor)}\"\n\n        # Adjusting the tick labels to reflect the original image size\n        ax.xaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(format_func_x))\n        ax.yaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(format_func_y))\n\n\n    plt.tight_layout()\n    plt.show()\n\n\n# Example usage:\n# Assuming `image_BGR` and `image_RGB` are numpy arrays of images or string paths to the images\n# show_images(image_BGR, image_RGB, titles=[\"BGR Image\", \"RGB Image\"])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-02-19T23:02:37.060582Z","iopub.execute_input":"2024-02-19T23:02:37.0612Z","iopub.status.idle":"2024-02-19T23:02:37.079157Z","shell.execute_reply.started":"2024-02-19T23:02:37.061169Z","shell.execute_reply":"2024-02-19T23:02:37.077775Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Image Analisys","metadata":{}},{"cell_type":"markdown","source":"Read the image from disk.\nThe image has been created with DALL-E with prompt:\n\n\"Image capturing the serene beauty of an idyllic scene in Africa, featuring a majestic white elephant near the edge of a blue lake, surrounded by lush trees with green leaves under a bright yellow sun in a clear sky.\"","metadata":{}},{"cell_type":"code","source":"image_BGR = cv2.imread(path_image)\nprint(f'Image: {path_image}')\nprint(f'Size of image: {image_BGR.shape}')","metadata":{"id":"NpC-_Nh5OBHI","execution":{"iopub.status.busy":"2024-02-19T23:02:37.0818Z","iopub.execute_input":"2024-02-19T23:02:37.082406Z","iopub.status.idle":"2024-02-19T23:02:37.145434Z","shell.execute_reply.started":"2024-02-19T23:02:37.082351Z","shell.execute_reply":"2024-02-19T23:02:37.144173Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Convert an image from BGR (Blue, Green, Red) color space to RGB (Red, Green, Blue).<br/>\nThis conversion is necessary because OpenCV loads images in BGR mode by default,<br/>\nbut for most image processing and visualization tasks, the RGB format is preferred.\n","metadata":{}},{"cell_type":"code","source":"image_RGB = cv2.cvtColor(image_BGR, cv2.COLOR_BGR2RGB)\nimage_RGB_original = image_RGB.copy() #save a backup of the original image\n\nshow_images(image_BGR, \n            image_RGB, \n            titles=[\"BGR\", \"RGB\"], \n            max_resolution=800)   ","metadata":{"execution":{"iopub.status.busy":"2024-02-19T23:02:37.147552Z","iopub.execute_input":"2024-02-19T23:02:37.148037Z","iopub.status.idle":"2024-02-19T23:02:38.631532Z","shell.execute_reply.started":"2024-02-19T23:02:37.148006Z","shell.execute_reply":"2024-02-19T23:02:38.630198Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nNormalize the size of the image to not affect the results. \nWhen image is too large, the processing of subsequent image processing tasks is compromised.\nWhen it is too small, details may not be noticed.","metadata":{}},{"cell_type":"code","source":"image_RGB, scaling_factor = resize_image(image_RGB, max_resolution=2000)\nshow_images(image_RGB_original, \n            image_RGB, \n            titles=[\"original size\", \n                    f\"scale to {scaling_factor*100:.0f}%\"], \n            max_resolution=800)   ","metadata":{"execution":{"iopub.status.busy":"2024-02-19T23:02:38.633146Z","iopub.execute_input":"2024-02-19T23:02:38.633516Z","iopub.status.idle":"2024-02-19T23:02:40.145299Z","shell.execute_reply.started":"2024-02-19T23:02:38.633484Z","shell.execute_reply":"2024-02-19T23:02:40.141994Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Color space conversions\nFirst step is to convert the image to color space HSV: hue, saturation, value. <br/>\nThe hue channel is the most important for detect objects with distinct colors. <br/>\nThe saturation channel is useful to separate colorful objects.  <br/>\nThe value channel refer to the brightness of the object.<br/>","metadata":{}},{"cell_type":"markdown","source":"## HSV\n<img src=\"https://miro.medium.com/v2/resize:fit:786/format:webp/1*qTdRziMFVUMBhKb11cm0qA.png\" style=\"clip-path: inset(0 50% 0 0);\">\n","metadata":{}},{"cell_type":"code","source":"image_HSV = cv2.cvtColor(image_RGB, cv2.COLOR_RGB2HSV)\nshow_images(image_HSV[:,:,0], image_HSV[:,:,1], image_HSV[:,:,2], titles=[\"Channel 0: HUE\", \"Channel 1: Saturation\", \"Channel 2: Value\"])","metadata":{"execution":{"iopub.status.busy":"2024-02-19T23:02:40.148801Z","iopub.execute_input":"2024-02-19T23:02:40.149265Z","iopub.status.idle":"2024-02-19T23:02:41.815351Z","shell.execute_reply.started":"2024-02-19T23:02:40.149224Z","shell.execute_reply":"2024-02-19T23:02:41.813484Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Grayscale and CLAHE\n\n<p>\nThe V channel (Value) of the HSV (Hue, Saturation, Value) color space represents the brightness of a color, which makes it a good approximation for grayscale because it captures the luminance of the image. This is in contrast to converting an image to grayscale using the RGB (Red, Green, Blue) color channels, which involves a different approach that might not always capture perceived brightness accurately.\n\n<p>\n**Perceptual Luminance Representation:**\n<br/>\nThe V channel in HSV is designed to represent the brightness or intensity of an image, closely aligning with human perception of brightness. This makes it a good candidate for representing an image in grayscale, as it captures the perceptual importance of different colors' luminance.\n\n<p>\n<br/>\n**Color Saturation and Hue Independence:**\n<br/>\nSince the V channel is separate from hue and saturation, it provides a pure measure of brightness, unaffected by the color's hue or saturation. This separation can be advantageous when the color information is less important than the brightness levels for analysis or processing tasks.\n\n<p>\n<br/>\n**CLAHE**\nContrast Limited Adaptive Histogram Equalization (CLAHE) is an advanced method used in computer vision to improve the contrast of images. It is an extension of adaptive histogram equalization (AHE), specifically designed to prevent the overamplification of noise that AHE tends to cause in homogeneous areas of an image. CLAHE operates by improving the local contrast and bringing out details in images, which can significantly aid various computer vision tasks. \n\nThe improved local contrast and detail enhancement provided by CLAHE can lead to more accurate and reliable feature detection. Features such as edges, corners, and textures become more pronounced, making it easier for computer vision algorithms (like object detection, edge detection, and feature matching) to perform their tasks effectively.","metadata":{}},{"cell_type":"code","source":"#extract grayscale from HS(V)\nimage_HSV = cv2.cvtColor(image_RGB, cv2.COLOR_RGB2HSV)\nimage_GRAY = image_HSV[:,:,2]\n\n#apply CLAHE to the image\nclahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(18, 18))\nimage_GRAY_clahe = clahe.apply(image_GRAY)\n\nshow_images(image_GRAY, image_GRAY_clahe, titles=[\"Original\", \"CLAHE\"])","metadata":{"execution":{"iopub.status.busy":"2024-02-19T23:02:41.816981Z","iopub.execute_input":"2024-02-19T23:02:41.817369Z","iopub.status.idle":"2024-02-19T23:02:43.008665Z","shell.execute_reply.started":"2024-02-19T23:02:41.817337Z","shell.execute_reply":"2024-02-19T23:02:43.007325Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LAB (CIELAB)\n\n<img src='https://sensing.konicaminolta.us/wp-content/uploads/ColorSphere.jpg'>\n<br/>\nSource: https://sensing.konicaminolta.us/mx/blog/entendiendo-el-espacio-de-color-cie-lab/\n","metadata":{}},{"cell_type":"code","source":"image_LAB = cv2.cvtColor(image_RGB, cv2.COLOR_RGB2LAB)\n\nshow_images(image_LAB[:,:,0], image_LAB[:,:,1], image_LAB[:,:,2], titles=[\"Channel 0: L (Lightness)\", \"Channel 1: A (green–red color)\", \"Channel 2: B (blue–yellow color)\"])","metadata":{"execution":{"iopub.status.busy":"2024-02-19T23:02:43.010341Z","iopub.execute_input":"2024-02-19T23:02:43.010787Z","iopub.status.idle":"2024-02-19T23:02:44.676871Z","shell.execute_reply.started":"2024-02-19T23:02:43.010746Z","shell.execute_reply":"2024-02-19T23:02:44.675715Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It is possible to filter pixels based on its color. <br/>\nIn this case we will use the second channel (B)<br/>\nto distinguish blue from yellow color.<br/>","metadata":{}},{"cell_type":"code","source":"# In LAB, negative values means that color is blue, and positive values means that color is yellow,\n# However, the cv2 returned the values as uint8 (unsigned), so we need to convert them to int8.\nimage_LAB_blue = image_LAB[:,:,2].astype(float)-128\n\n# Then we apply the threshold to the image\nimage_LAB_blue = np.where(image_LAB_blue < 0, 255, 0).astype(np.uint8)\n\nshow_images(image_RGB, image_LAB_blue, \n            titles=[\"Image with Original colors\", \"Blue part of the image\"])","metadata":{"execution":{"iopub.status.busy":"2024-02-19T23:02:44.678353Z","iopub.execute_input":"2024-02-19T23:02:44.678909Z","iopub.status.idle":"2024-02-19T23:02:45.79199Z","shell.execute_reply.started":"2024-02-19T23:02:44.678878Z","shell.execute_reply":"2024-02-19T23:02:45.790506Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The same can be used with channel A (green–red color).\nHowever, many variances of blue contains green,\nso we may need a lower threshold like -20 to select leafs.","metadata":{}},{"cell_type":"code","source":"# In LAB, negative values means that color is green, and positive values means that color is red,\n# However, the cv2 returned the values as uint8 (unsigned), so we need to convert them to int8.\nimage_LAB_green = image_LAB[:,:,1].astype(float)-128\n\n# Then we apply the threshold to the image\nimage_LAB_with_green = np.where(image_LAB_green < 0, 255, 0).astype(np.uint8)\nimage_LAB_with_more_green = np.where(image_LAB_green < -20, 255, 0).astype(np.uint8)\n\nshow_images(image_RGB, image_LAB_with_green, image_LAB_with_more_green, \n            titles=[\"Image with Original colors\", \"Green part of the image\", \"Part of the image with more green\"])","metadata":{"execution":{"iopub.status.busy":"2024-02-19T23:02:45.793384Z","iopub.execute_input":"2024-02-19T23:02:45.794213Z","iopub.status.idle":"2024-02-19T23:02:47.33727Z","shell.execute_reply.started":"2024-02-19T23:02:45.79418Z","shell.execute_reply":"2024-02-19T23:02:47.336018Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fast-GLCM","metadata":{"id":"jbZI2VCqMXZj"}},{"cell_type":"markdown","source":"## fast_glcm functions\nhttps://github.com/tzm030329/GLCM\nModified by LeonanUCM","metadata":{}},{"cell_type":"code","source":"# coding: utf-8\n# https://github.com/tzm030329/GLCM\n# Modified by LeonanUCM\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nfrom skimage import data\n\n\ndef main():\n    pass\n\n\ndef fast_glcm(img, vmin=0, vmax=255, levels=8, kernel_size=5, distance=1.0, angle=0.0, kernel_shape=\"square\"):\n    '''\n    Parameters\n    ----------\n    img: array_like, shape=(h,w), dtype=np.uint8\n        input image\n    vmin: int\n        minimum value of input image\n    vmax: int\n        maximum value of input image\n    levels: int\n        number of grey-levels of GLCM\n    kernel_size: int\n        Patch size to calculate GLCM around the target pixel\n    distance: float\n        pixel pair distance offsets [pixel] (1.0, 2.0, and etc.)\n    angle: float\n        pixel pair angles [degree] (0.0, 30.0, 45.0, 90.0, and etc.)\n\n    Returns\n    -------\n    Grey-level co-occurrence matrix for each pixels\n    shape = (levels, levels, h, w)\n    '''\n\n    mi, ma = vmin, vmax\n    ks = kernel_size\n    h,w = img.shape\n\n    # digitize\n    bins = np.linspace(mi, ma+1, levels+1)\n    gl1 = np.digitize(img, bins) - 1\n\n    # make shifted image\n    dx = distance*np.cos(np.deg2rad(angle))\n    dy = distance*np.sin(np.deg2rad(-angle))\n    mat = np.array([[1.0,0.0,-dx], [0.0,1.0,-dy]], dtype=np.float32)\n    gl2 = cv2.warpAffine(gl1, mat, (w,h), flags=cv2.INTER_NEAREST,\n                         borderMode=cv2.BORDER_REPLICATE)\n\n    # make glcm\n    glcm = np.zeros((levels, levels, h, w), dtype=np.uint8)\n    for i in range(levels):\n        for j in range(levels):\n            mask = ((gl1==i) & (gl2==j))\n            glcm[i,j, mask] = 1\n\n    if kernel_shape == \"square\":\n        kernel = np.ones((ks, ks), dtype=np.uint8)\n    else: #circle\n        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (ks, ks))\n\n    for i in range(levels):\n        for j in range(levels):\n            glcm[i,j] = cv2.filter2D(glcm[i,j], -1, kernel)\n\n    glcm = glcm.astype(np.float32)\n    return glcm\n\n\ndef fast_glcm_mean(img, vmin=0, vmax=255, levels=8, ks=5, distance=1.0, angle=0.0, precalculated_glcm=None):\n    '''\n    calc glcm mean\n    '''\n    h,w = img.shape\n    \n    if precalculated_glcm: \n        glcm, vmin, vmax, levels, ks, distance, angle = precalculated_glcm\n    else:\n        glcm = fast_glcm(img, vmin, vmax, levels, ks, distance, angle)\n\n    mean = np.zeros((h,w), dtype=np.float32)\n    for i in range(levels):\n        for j in range(levels):\n            mean += glcm[i,j] * i / (levels)**2\n\n    return mean\n\n\ndef fast_glcm_std(img, vmin=0, vmax=255, levels=8, ks=5, distance=1.0, angle=0.0, precalculated_glcm=None):\n    '''\n    calc glcm std\n    '''\n    h,w = img.shape\n\n    if precalculated_glcm: \n        glcm, vmin, vmax, levels, ks, distance, angle = precalculated_glcm\n    else:\n        glcm = fast_glcm(img, vmin, vmax, levels, ks, distance, angle)\n\n    mean = np.zeros((h,w), dtype=np.float32)\n    for i in range(levels):\n        for j in range(levels):\n            mean += glcm[i,j] * i / (levels)**2\n\n    std2 = np.zeros((h,w), dtype=np.float32)\n    for i in range(levels):\n        for j in range(levels):\n            std2 += (glcm[i,j] * i - mean)**2\n\n    std = np.sqrt(std2)\n    return std\n\n\ndef fast_glcm_contrast(img, vmin=0, vmax=255, levels=8, ks=5, distance=1.0, angle=0.0, precalculated_glcm=None):\n    '''\n    calc glcm contrast\n    '''\n\n    h,w = img.shape\n    if precalculated_glcm: \n        glcm, vmin, vmax, levels, ks, distance, angle = precalculated_glcm\n    else:\n        glcm = fast_glcm(img, vmin, vmax, levels, ks, distance, angle)\n\n    cont = np.zeros((h,w), dtype=np.float32)\n    for i in range(levels):\n        for j in range(levels):\n            cont += glcm[i,j] * (i-j)**2\n\n    \n    return cont\n\n\ndef fast_glcm_dissimilarity(img, vmin=0, vmax=255, levels=8, ks=5, distance=1.0, angle=0.0, precalculated_glcm=None):\n    '''\n    calc glcm dissimilarity\n    '''\n    h,w = img.shape\n\n    if precalculated_glcm: \n        glcm, vmin, vmax, levels, ks, distance, angle = precalculated_glcm\n    else:\n        glcm = fast_glcm(img, vmin, vmax, levels, ks, distance, angle)\n\n    diss = np.zeros((h,w), dtype=np.float32)\n    for i in range(levels):\n        for j in range(levels):\n            diss += glcm[i,j] * np.abs(i-j)\n    \n    return diss\n\n\ndef fast_glcm_homogeneity(img, vmin=0, vmax=255, levels=8, ks=5, distance=1.0, angle=0.0, precalculated_glcm=None):\n    '''\n    calc glcm homogeneity\n    '''\n    h,w = img.shape\n\n    if precalculated_glcm: \n        glcm, vmin, vmax, levels, ks, distance, angle = precalculated_glcm\n    else:\n        glcm = fast_glcm(img, vmin, vmax, levels, ks, distance, angle)\n\n    homo = np.zeros((h,w), dtype=np.float32)\n    for i in range(levels):\n        for j in range(levels):\n            homo += glcm[i,j] / (1.+(i-j)**2)\n\n    return homo\n\n\ndef fast_glcm_ASM(img, vmin=0, vmax=255, levels=8, ks=5, distance=1.0, angle=0.0, precalculated_glcm=None):\n    '''\n    calc glcm asm, energy\n    '''\n    h,w = img.shape\n\n    if precalculated_glcm: \n        glcm, vmin, vmax, levels, ks, distance, angle = precalculated_glcm\n    else:\n        glcm = fast_glcm(img, vmin, vmax, levels, ks, distance, angle)\n\n    asm = np.zeros((h,w), dtype=np.float32)\n    for i in range(levels):\n        for j in range(levels):\n            asm  += glcm[i,j]**2\n\n    ene = np.sqrt(asm)\n    return asm#, ene\n\n\ndef fast_glcm_max(img, vmin=0, vmax=255, levels=8, ks=5, distance=1.0, angle=0.0, precalculated_glcm=None):\n    '''\n    calc glcm max\n    '''\n\n    if precalculated_glcm: \n        glcm, vmin, vmax, levels, ks, distance, angle = precalculated_glcm\n    else:\n        glcm = fast_glcm(img, vmin, vmax, levels, ks, distance, angle)\n        \n    max_  = np.max(glcm, axis=(0,1))\n    return max_\n\n\ndef fast_glcm_entropy(img, vmin=0, vmax=255, levels=8, ks=5, distance=1.0, angle=0.0, precalculated_glcm=None):\n    '''\n    calc glcm entropy\n    '''\n\n    if precalculated_glcm: \n        glcm, vmin, vmax, levels, ks, distance, angle = precalculated_glcm\n    else:\n        glcm = fast_glcm(img, vmin, vmax, levels, ks, distance, angle)\n\n    pnorm = glcm / np.sum(glcm, axis=(0,1)) + 1./ks**2\n    ent  = np.sum(-pnorm * np.log(pnorm), axis=(0,1))\n    return ent","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-02-19T23:02:47.339094Z","iopub.execute_input":"2024-02-19T23:02:47.339453Z","iopub.status.idle":"2024-02-19T23:02:48.007182Z","shell.execute_reply.started":"2024-02-19T23:02:47.339421Z","shell.execute_reply":"2024-02-19T23:02:48.00617Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test GLCM","metadata":{}},{"cell_type":"code","source":"levels = 3\nkernel_size = 10\nvmin, vmax = 0, 255\ndistance = 1.0\nangle = 0.0\n\nprint(f'Calculating base GLCM matrix...')\nprecalculated_glcm=(fast_glcm(image_GRAY, \n                              vmin, vmax, levels, kernel_size, distance, angle), \n                              vmin, vmax, levels, kernel_size, distance, angle)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T23:02:48.010643Z","iopub.execute_input":"2024-02-19T23:02:48.011395Z","iopub.status.idle":"2024-02-19T23:02:48.863672Z","shell.execute_reply.started":"2024-02-19T23:02:48.011359Z","shell.execute_reply":"2024-02-19T23:02:48.86217Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Each GLCM filter is used to highlight specific features, for example most homogeneous part of image: fast_glcm_homogeneity.\nThe method OTSU is used to select the threshold that better splits images into two clases based on filter aplied.","metadata":{}},{"cell_type":"code","source":"# First create a list with each GLCM method available (functions)\nmethods = [['Contrast',      fast_glcm_contrast], \n           ['Dissimilarity', fast_glcm_dissimilarity],\n           ['Homogeneity',   fast_glcm_homogeneity],\n           ['Entropy',       fast_glcm_entropy],\n           ['ASM',           fast_glcm_ASM], \n           ['Mean',          fast_glcm_mean], \n           ['Std',           fast_glcm_std], \n           ['Max',           fast_glcm_max]] \n\nfor method_glcm, function_glcm in methods:\n    # Calculates each GLCM method\n    result_glcm = function_glcm(image_GRAY_clahe, precalculated_glcm=precalculated_glcm)\n    \n    # Normaliza datos from 0-255\n    min_glcm, max_glcm = result_glcm.min(), result_glcm.max()\n    if min_glcm != max_glcm:\n        result_glcm_normal = ((result_glcm - min_glcm) / (max_glcm - min_glcm)*255).astype(np.uint8)\n        threshold = threshold_otsu(result_glcm)\n        threshold_pctg = (threshold-min_glcm)/(max_glcm-min_glcm)*100\n    else:\n        # Avoid division by zero\n        result_glcm_normal = result_glcm * 0\n        threshold, threshold_pctg = 0, 0\n    \n    print(f'\\nMethod={method_glcm}:   Min={min_glcm}   Max={max_glcm}   Threshold={threshold:.3f} ({threshold_pctg:.1f}%)')\n        \n    # Binarize the result based on optimal division by OTSU\n    mask_binary = np.where(result_glcm > threshold, \n                           255, \n                           0).astype(np.uint8)\n    \n    # Apply the mask to the original image to obtain the ROI\n    image_selected = cv2.bitwise_and(image_RGB, image_RGB, mask=~mask_binary)\n\n    # Display result\n    show_images(result_glcm_normal, mask_binary, image_RGB, image_selected,\n                titles=[\"Normalized Mask\", \"Binary Mask\", \"Original Image\", \"Selected Parts\"])    ","metadata":{"execution":{"iopub.status.busy":"2024-02-19T23:02:48.865525Z","iopub.execute_input":"2024-02-19T23:02:48.865925Z","iopub.status.idle":"2024-02-19T23:03:07.90129Z","shell.execute_reply.started":"2024-02-19T23:02:48.865882Z","shell.execute_reply":"2024-02-19T23:03:07.899072Z"},"trusted":true},"outputs":[],"execution_count":null}]}